\section{Background}
\label{sect.background}

Distributed systems are conventionally modeled as a set of \emph{nodes} (or \emph{processes}) that
can communicate only by sending and receiving messages over a network. In practice, nodes may be
servers in datacenters, end-user devices such as laptops and smartphones, or any other kind of
device with network connectivity. We assume that messages sent via the network may be delayed,
reordered, or lost entirely---an assumption that matches the behaviour of many networks in practice
\cite{Bailis:2014jx}.

Moreover, nodes or network links may fail at any time, and the non-faulty parts of the system need
to continue working in spite of such partial failures. Programming such systems is far more
difficult than writing programs that execute on a single machine. To simplify the programming model,
application developers may rely on abstractions that guarantee certain properties in all possible
executions of a system, even in the face of faults.

In this paper, the abstraction we discuss is \emph{replicated data}, that is, ensuring that each
node has a local copy of some data structure (such as a document). A node that has a copy of the
data is called a \emph{replica}. Each node may independently read and modify the data, and a
replication algorithm propagates any updates (modifications) to other replicas via the network.

\subsection{Properties of Networks}\label{sect.background.networks}

Various replication algorithms provide different consistency guarantees, and the most appropriate
choice depends on the properties of the network. For example, many online services today rely on a
central server that maintains the authoritative copy of the data. In this architecture, clients that
want to modify the data send their updates to the central server (often called \emph{master} or
\emph{leader}), which in turn forwards the updates to all of the replicas. The server can attach a
sequence number to each update, enabling all replicas to apply the updates in the same order. This
notion of total ordering enables strong consistency models such as linearizability
\cite{Herlihy:1990jq} and serializable transactions.

However, peer-to-peer networks have no such central server that can be assumed to be always
reachable. It is possible to provide the same guarantee of totally ordered delivery in peer-to-peer
networks; this abstraction is known as \emph{total order broadcast} or \emph{atomic broadcast}
\cite{Cachin:2011wt,Defago:2004ji}. Unfortunately, it is expensive: it requires a quorum of nodes
(typically a majority) to be reachable in order to make progress \cite{Chandra:1996cp}; in a
peer-to-peer network of mobile devices, it is likely that less than a majority of devices is online
at any one time, and so any algorithm for total order broadcast would be stalled.

As discussed in Section~\ref{sect.introduction}, we aim to support scenarios in which subsets of
nodes can communicate with each other, but some are disconnected (\emph{partitioned}) from others.
For example, consider some data shared between two users, each of whom has two devices. Each user
may want to synchronise the data between their two devices using a local wireless link, even if they
are temporarily disconnected from the internet, and re-synchronize with the other user when they are
next online. In this scenario, strong consistency across both users could only be achieved by
preventing users from making any offline updates; if updates are allowed, weaker consistency is
inevitable \cite{Attiya:2015dm,Davidson:1985hv}.

We model this kind of system as an \emph{asynchronous} network, which means that we make no timing
assumptions: messages sent over the network may suffer unbounded delays before they are delivered,
or they may never arrive at all. Nodes may pause their execution for unbounded periods of time, or
fail permanently \cite{Cachin:2011wt}. If a user makes updates while offline, and the device
re-synchronises when it is next online, we can simply model that interaction as very large network
delay. Our formal definition of the asynchronous model appears in Section~\ref{sect.network}.

Although most practical systems have fairly low network delay and fairly brief execution pauses most
of the time, the asynchronous model is a useful lower bound for the assumptions that a distributed
algorithm may make. Any algorithm that is proved correct under the assumptions of the asynchronous
model is also guaranteed to be correct in a system that provides stronger guarantees, for example
around timing and reliability.

\subsection{Strong Eventual Consistency (SEC)}\label{sect.eventual.consistency}

When replicas concurrently modify their local copy of the data, their states may diverge. In
peer-to-peer networks such as those described in Section~\ref{sect.background.networks}, enforcing
strong consistency would cause an application to become unavailable. Instead, we can only make a
weaker guarantee, namely that replicas \emph{eventually} converge to a consistent state.

A frequently-used definition of \emph{eventual consistency} is: ``if no new updates are made to an
object, eventually all reads will return the last updated value''
\cite{Bailis:2013jc,Burckhardt:2014hy,Terry:1994fp,Vogels:2009ca}. However, this property is very
weak: it does not define what happens if the updates to the object never cease, nor does it
constrain the values that reads may return before consistency is eventually reached.

\emph{Strong eventual consistency} (SEC) is a more refined definition that we can use in this
context \cite{Shapiro:2011un}. It requires \emph{convergence} of replicas, which is defined
informally as: ``whenever any two replicas have delivered the same set of updates, they have
equivalent state'' (we present our formal definition in Section~\ref{sect.convergence}). Two states
are equivalent if all reads return the same result in both states. A replica can perform reads and
writes using only its local state, without waiting for network communication, and exchange updates
with other replicas asynchronously when a network connection is available. This definition
constrains the values that reads may return at any time, making it a stronger property than eventual
consistency.

If network interruptions between nodes last only for a finite time, every update can be eventually
delivered to every non-failed node by re-sending messages until they arrive. Thus, all non-failed
nodes will eventually deliver the same set of updates (although perhaps in a different order), and
so SEC will ensure that they all converge to an equivalent state. If nodes are partitioned from each
other forever, they will not converge, but that is the case for any replication algorithm, since the
network is assumed to be the only communication channel.
