\documentclass[acmlarge,review,anonymous]{acmart}\settopmatter{printfolios=true}
%\documentclass[acmlarge]{acmart}\settopmatter{}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}
\usepackage[english]{babel}
\usepackage{setspace}

\usepackage{paralist} % For inline enumeration
\usepackage{tikz} % For diagrams
\usetikzlibrary{arrows}

\usepackage{isabelle,isabellesym}
\isabellestyle{it}

\newif\ifextended
\extendedfalse

\hyphenation{App-Jet}

%% PACMPL journal information will be supplied by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{--}
\acmNumber{--}
\acmArticle{0}
\acmYear{2017}
\acmMonth{4}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\setcopyright{none}

\begin{document}
\title{Verifying Strong Eventual Consistency in Distributed Systems}

%% Each author should be introduced by \author, followed by \authornote (optional),
%% \orcid (optional), \affiliation, and \email.
%% An author may have multiple affiliations and/or emails; repeat the appropriate command.
%% Many elements are not rendered, but should be provided for metadata extraction tools.
\author{Victor B.\ F.\ Gomes}
%\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{vb358@cam.ac.uk}

\author{Martin Kleppmann}
\orcid{0000-0001-7252-6958}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{mk428@cam.ac.uk}

\author{Dominic P.\ Mulligan}
%\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{dpm36@cam.ac.uk}

\author{Alastair R.\ Beresford}
\orcid{0000-0003-0818-6535}
\affiliation{
  \position{Senior Lecturer}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{arb33@cam.ac.uk}

\begin{abstract}
Data replication is used in distributed systems to maintain an up-to-date copy of some data across multiple computers in a network.
However, despite decades of research, algorithms for achieving replication in distributed systems are still poorly understood.
Indeed, many published algorithms have later been shown to be incorrect, even those accompanied by supposed mechanised proofs of correctness.
In this work, we focus on the correctness of Conflict-free Replicated Data Types (CRDTs), a class of algorithm that provides strong eventual consistency guarantees for replicated data.
We develop a modular and reusable framework in the Isabelle/HOL interactive theorem prover for verifying the correctness of CRDT implementations.
We avoid correctness issues that have dogged previous mechanised proofs in this area by taking a new approach: axiomatically modelling real-world computer networks using six uncontroversial axioms.
Our framework builds on the network model to obtain a machine-checked assurance that our theorems hold in all possible network behaviours.
We identify an abstract convergence theorem, a property of order relations, which provides a formal definition of strong eventual consistency.
We then obtain the first machine-checked correctness theorems for three concrete CRDTs: the replicated growable array, the observed-removed set, and a increment-decrement counter.
We find that our framework is highly reusable, developing proofs of the latter two CRDTs in a few hours and with relatively little CRDT-specific code.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003033.10003039.10003041.10003042</concept_id>
<concept_desc>Networks~Protocol testing and verification</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010537.10010540</concept_id>
<concept_desc>Computer systems organization~Peer-to-peer architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003033.10003039.10003041.10003043</concept_id>
<concept_desc>Networks~Formal specifications</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010172</concept_id>
<concept_desc>Theory of computation~Distributed algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10010124.10010138.10010142</concept_id>
<concept_desc>Theory of computation~Program verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011099.10011692</concept_id>
<concept_desc>Software and its engineering~Formal software verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Networks~Protocol testing and verification}
\ccsdesc[500]{Computer systems organization~Peer-to-peer architectures}
\ccsdesc[300]{Networks~Formal specifications}
\ccsdesc[300]{Theory of computation~Distributed algorithms}
\ccsdesc[300]{Theory of computation~Program verification}
\ccsdesc[300]{Software and its engineering~Formal software verification}

\keywords{strong eventual consistency, verification, distributed systems, replication, convergence, CRDTs, automated theorem proving}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sect.introduction}

A data replication algorithm is executed by a set of computers---or \emph{nodes}---in a distributed system and ensures that all nodes eventually obtain an identical copy of some shared state.
Whilst vital for overall systems correctness, correctly implementing a replication algorithm is a challenging task, as any such algorithm must operate above computer networks which may arbitrarily delay, drop, or reorder messages, suffer from a temporary partition of the nodes, or even observe outright node failure.

Reflecting the importance of the task, a number of replication algorithms exist, with different algorithms exploring an inherent trade-off between the strength of data consistency guarantees provided across nodes versus operational constraints and performance.
Accordingly, replication algorithms can be divided into three classes---\emph{strong consistency}, \emph{eventual consistency}, and \emph{strong eventual consistency}---based on the data consistency guarantees provided.

Strong consistency can be understood as linearizability, in that the system behaves as if there were only a single copy of the data, even when it is replicated.
Traditional relational databases, such as PostgreSQL, use this model: locks are used to ensure that all users connected to the database have an identical view of its content at all times.

Depending on the domain, a strong consistency model may be unwarranted, cause unacceptable performance degradation, or simply be infeasible to implement in large distributed systems.
This motivates models with relaxed consistency guarantees compared to those provided by strong consistency, but perhaps better suited to certain application domains where changes to shared state need not be propagated eagerly, and with better performance characteristics.
One popular relaxed consistency model is eventual consistency.
Eventual consistency guarantees that, when no new updates are made to the shared state, all nodes will eventually agree on the contents of the shared state.
Distributed version control systems, such as Git or Mercurial, use this model for example, as do many distributed database systems.

The guarantees offered by the eventual consistency model are weak---the model does not constrain system behaviour in the case where updates to shared state never cease, nor does it constrain the values of intermediate reads from shared state prior to convergence.
Certain applications therefore require models that provide weaker guarantees than those provided by strong consistency, but stronger guarantees than those provided by eventual consistency.
One such model is strong eventual consistency, which guarantees that whenever two nodes have received the same set of messages (possibly in a different order), then the shared state on the two nodes will be identical.
Data centre applications built with Riak, and collaborative editing systems such as Google Docs, use this model for example.

In this paper we focus on replication algorithms which support decentralised operation and allow modifications to shared state in the presence of arbitrary network partitions.
Such algorithms support applications running on mobile devices such as laptops and smartphones which are not always connected to the Internet, but are instead completely disconnected or only have a local connection (e.g. via Bluetooth) to a subset of the nodes containing the shared state.
Example applications built using such algorithms could include collaborative editing systems such as text documents or spreadsheets where several users need to work on the same document, or data synchronisation tasks such as shared calendars, address books or note-taking tools.

We argue that such systems are best designed with replication algorithms which provide strong eventual consistency. Strong consistency algorithms are inappropriate in this context as they cannot support local updates on nodes in the presence of arbitrary network partitions.
In other words, editing of a spreadsheet by a node would not be allowed without network connectivity to a majority of nodes.
Eventual consistency is inappropriate, as it requires the user to manually resolve merge conflicts.
In contrast replication algorithms which support strong eventual consistency work in a distributed setting and are, by definition, conflict free.

Nevertheless, eventual consistency is how many popular collaborative and data synchronisation applications work today: allow
conflicts to arise and provide the user with mechanisms for resolving them.
The other common solution is to use a strong eventual consistency algorithm with a central server, for example, as popularised by Google Docs.
This approach does not support offline editing: it requires the user's computer is connected to the server in order to ensure updates are handled in a conflict-free manner.
Furthermore, it requires users trust that the central server is not compromised by attackers or subverted by a malicious insider who could tamper with the data or grant access to unauthorised parties.
Finally, a central server is a single point of failure that is susceptible to denial-of-service attacks, blocking and censorship.

Decentralised, or peer-to-peer systems are attractive in scenarios where reliance on a central server is impractical or undesirable.
This includes both the mobile application scenarios described above, as well as data centre settings in which a central server is a bottleneck, limiting scalability.
Unfortunately, we currently have a poor understanding of algorithms that enable strong eventual consistency in this setting, and this is limiting their use and deployment.
In Section~\ref{sect.relatedwork} we highlight several decentralised strong eventual consistency algorithms, published in peer-reviewed venues, that claimed to work correctly but were subsequently shown to violate their supposed guarantees. 
Informal reasoning has resulted in the development of algorithms later shown incorrect; even formal, mechanised proofs of algorithms have later been shown incorrect.
These proofs failed because the proofs were shallow: axioms which were thought to be correct were later found to be wrong.

Given the failure of previous mechanised proofs we take a different approach: rather than writing axioms which talk about any given strong eventual consistency algorithm we write down the axioms of a realistic class of computer network, the \emph{asynchronous causal network}.
This network model describes the behviour of large-scale computer networks today, which may reorder and delay message delivery (simulating temporary network partitions), as well as drop messages entirely (simulating permanent network and computer failure).
We use Isabelle/HOL, a generic proof assistant tool~\cite{DBLP:conf/tphol/WenzelPN08}, to create formal specifications of the network and distributed algorithms executing in such a system. 
We then use this framework to produce machine-checked proofs of correctness of a class of strong eventual consistency algorithms called Conflict-Free Replicated Data Types (CRDT) as introduced by~\citet{Shapiro:2011wy,Shapiro:2011un}. 

Our ultimate goal is to formally verify the correctness of a suite of distributed data types which can be composed and encapsulated in a library for use by application developers, thus providing an easy way to produce applications which work in a distributed setting and providing end users with a new generation of conflict-free collaborative and data synchronising applications.

Our contributions in this paper are as follows:
\begin{itemize}
\item We outline an approach for proving the correctness of strong eventual consistency algorithms, namely by building a model of asynchronous causal networks, a communication paradigm supported by virtually all computer network technologies today.
%
\item We develop the first, modular and reusable framework for verifying the correctness of strong eventual consistency algorithms, obtaining a machine-checked assurance that our theorems hold in all possible network behaviours.
%
\item We identify an abstract convergence theorem, a property of order relations, with which we can obtain correctness theorems for concrete strong eventual consistency algorithms.
%
\item We provide the first mechanised proofs of the Replicated Growable Array, the Observed-Removed Set and increment-decrement counter CRDTs. 
%
\item We demonstrate our framework is highly reusable: we developed proofs of the Observed-Removed Set and a increment-decrement counter CRDTs in a few hours and with little CRDT-specific code.
\end{itemize}

\input{background}
\input{relwork}
\input{isabelle}
\input{convergence}
\input{network}
\input{rga}
\input{simple-crdts}

\section{Conclusion}
\label{sect.conclusion}

In this paper we introduced a new approach for proving the correctness of SEC algorithms: instead of defining axioms related to the basic properties of any individual algorithm, we instead build up from a formal, realistic model of a computer network which may delay, drop or reorder messages sent between computers.
We then complemented this model with an abstract specification of SEC.
We took this approach both to ease the burden of constructing proofs of new SEC algorithms, but also to address the difficulties with previous approaches where peer-reviewed proofs, including mechanised proofs, have later been shown to be incorrect.

We used our framework to show that three op-based CRDT algorithms do indeed provide the SEC property under all possible valid executions of our network model.
Our second and third proofs took only a few hours to write and under 100 lines of additional code, demonstrating that our framework is readily reusable.

All of our convergence proofs for our CRDT implementations follow a familiar recipe.
First local states, or a concrete implementation of the replicated type is defined along with a type of messages corresponding to the operations supported by the CRDT.
Together, these are coupled with an interpretation function which `decodes' messages and lifts them into a transformer of local states.
This transformer is used to define a CRDT-specific locale, a specialisation of the $\isa{network}{\isacharunderscore}\isa{with}{\isacharunderscore}(\isa{constrained}{\isacharunderscore})\isa{ops}$ locale, which may also provide restrictions on valid network behaviours in order to ensure convergence.
Next, all operations are shown to commute with themselves, and with each other, with preconditions on commutation being eventually dischargeable as a side-effect of the restrictions on network behaviours mentioned above.
Coupling our abstract convergence theorem with a CRDT-specific technical lemma showing that the concurrent delivered messages of a prefix of a local history of a node commute, it is easy to prove convergence.
In fact, the same single line of Isabelle proof is used to obtain the convergence theorem as a corollary, in all of our three concrete CRDT implementations.

As this recipe described above demonstrates, in this work we have not only isolated a series of reusable lemmas and models of networks, but also identified a fixed proof strategy that developers of new operation-based CRDTs can use to obtain a convergence theorem for their replicated type.
This idea is made precise with the $\isa{strong}{\isacharunderscore}\isa{eventual}{\isacharunderscore}\isa{consistency}$ local theory, which provides sufficient conditions in order to obtain a convergence theorem for a replicated type.

Our work was motivated by the desire to support a wider range of SEC algorithms in distributed systems.
OT algorithms which support the $\mathit{TP}_1$ alone are already widely used in systems such as Google Docs, however these require all clients communicate changes via a single central server.
Similarly, state-based CRDTs are used in systems such as Riak. Both of these approaches are limiting -- in the case of OT, the requirement for a central server prevents offline editing and direct collaboration via local WiFi; and in the case of state-based CRDTs, algorithms to support complex data structures such as ordered lists are yet to be found and in any case are likely impractical if small changes are regularly made to lists containing many items.
Consequently, our approach provides the groundwork for a new generation of applications which use truly distributed SEC alogrithms, including robust, collaborative applications working on complex data structures in a peer-to-peer setting, something which is likely to become increasingly important in a world where mobile devices are becoming ever more prevalent.

We speculate that our framework is also amenable to the inclusion of SEC algorithms based on OT as well as state-based CRDTs, and not limited to operation-based algorithms, which we have focussed on in this work.
Further, we also speculate that our framework could be used to demonstrate the equivalence of classes of SEC algorithms.
We leave this to future work.

%% contents of acknowledgement section are automatically suppressed when the 'anonymous' documentclass option is set
\begin{acks}
    %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and \grantnum[<url>]{<sponsorID>}{<number>}
    %% should be used to acknowledge financial support and will be used by metadata extraction tools.
    This research was supported by grants from
    \grantsponsor{GS100000003}{The Boeing Company}{http://dx.doi.org/10.13039/100000003} and
    \grantsponsor{GS501100000266}{EPSRC}{http://dx.doi.org/10.13039/501100000266} and
    \grantsponsor{EP/K008528}{EPSRC}{http://dx.doi.org/10.13039/501100000266}

    %TODO: REMS, Colleagues who have provided feedback.
\end{acks}

\bibliography{references}{}
\end{document}
