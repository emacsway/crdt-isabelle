\documentclass[acmlarge,anonymous,review]{acmart}\settopmatter{printfolios=true}
%\documentclass[acmlarge]{acmart}\settopmatter{}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}
\usepackage[english]{babel}
\usepackage{setspace}

\usepackage{paralist} % For inline enumeration
\usepackage{tikz} % For diagrams
\usetikzlibrary{arrows}

\usepackage{isabelle,isabellesym}
\isabellestyle{it}

\hyphenation{App-Jet}

%% PACMPL journal information will be supplied by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{--}
\acmNumber{--}
\acmArticle{0}
\acmYear{2017}
\acmMonth{4}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\setcopyright{none}

\begin{document}
\title{Verifying Strong Eventual Consistency in Distributed Systems}

%% Each author should be introduced by \author, followed by \authornote (optional),
%% \orcid (optional), \affiliation, and \email.
%% An author may have multiple affiliations and/or emails; repeat the appropriate command.
%% Many elements are not rendered, but should be provided for metadata extraction tools.
\author{Victor B.\ F.\ Gomes}
%\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{vb358@cam.ac.uk}

\author{Martin Kleppmann}
\orcid{0000-0001-7252-6958}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{mk428@cam.ac.uk}

\author{Dominic P.\ Mulligan}
%\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{dpm36@cam.ac.uk}

\author{Alastair R.\ Beresford}
\orcid{0000-0003-0818-6535}
\affiliation{
  \position{Senior Lecturer}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{arb33@cam.ac.uk}

\begin{abstract}
Data replication is used in distributed systems to maintain up-to-date copies of shared data across multiple computers in a network.
However, despite decades of research, algorithms for achieving consistency in replicated systems are still poorly understood.
Indeed, many published algorithms have later been shown to be incorrect, even some that were accompanied by supposed mechanised proofs of correctness.
In this work, we focus on the correctness of Conflict-free Replicated Data Types (CRDTs), a class of algorithm that provides strong eventual consistency guarantees for replicated data.
We develop a modular and reusable framework in the Isabelle/HOL interactive proof assistant for verifying the correctness of CRDT algorithms.
We avoid correctness issues that have dogged previous mechanised proofs in this area by including a network model in our formalisation, and proving that our theorems hold in all possible network behaviours.
Our axiomatic network model is a standard abstraction that accurately reflects the behaviour of real-world computer networks.
Moreover, we identify an abstract convergence theorem, a property of order relations, which provides a formal definition of strong eventual consistency.
We then obtain the first machine-checked correctness theorems for three concrete CRDTs: the Replicated Growable Array, the Observed-Remove Set, and an Increment-Decrement Counter.
We find that our framework is highly reusable, developing proofs of correctness for the latter two CRDTs in a few hours and with relatively little CRDT-specific code.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003033.10003039.10003041.10003042</concept_id>
<concept_desc>Networks~Protocol testing and verification</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010537.10010540</concept_id>
<concept_desc>Computer systems organization~Peer-to-peer architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003033.10003039.10003041.10003043</concept_id>
<concept_desc>Networks~Formal specifications</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010172</concept_id>
<concept_desc>Theory of computation~Distributed algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10010124.10010138.10010142</concept_id>
<concept_desc>Theory of computation~Program verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011099.10011692</concept_id>
<concept_desc>Software and its engineering~Formal software verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Networks~Protocol testing and verification}
\ccsdesc[500]{Computer systems organization~Peer-to-peer architectures}
\ccsdesc[300]{Networks~Formal specifications}
\ccsdesc[300]{Theory of computation~Distributed algorithms}
\ccsdesc[300]{Theory of computation~Program verification}
\ccsdesc[300]{Software and its engineering~Formal software verification}

\keywords{strong eventual consistency, verification, distributed systems, replication, convergence, CRDTs, automated theorem proving}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sect.introduction}

A data replication algorithm is executed by a set of computers---or \emph{nodes}---in a distributed system, and ensures that all nodes eventually obtain an identical copy of some shared state.
Whilst vital for overall systems correctness, implementing a replication algorithm is a challenging task, as any such algorithm must operate across computer networks that may arbitrarily delay, drop, or reorder messages, experience temporary partitions of the nodes, or even suffer outright node failure.
Reflecting the importance of this task, a number of replication algorithms exist, with different algorithms exploring the inherent trade-offs between the strength of data consistency guarantees, and operational characteristics such as scalability and performance.
Accordingly, replication algorithms can be divided into classes---\emph{strong consistency}, \emph{eventual consistency}, and \emph{strong eventual consistency}---based on the consistency guarantees that they provide.

Strong consistency can be understood as \emph{linearisability}, \emph{serialisability}, or a combination of the two (one-copy serialisability).
Intuitively, the goal of strong consistency is to make a system behave like a single sequentially executing node, even when it is replicated and concurrent.
Most systems implement strong consistency by designating a single node as the \emph{leader}, which decides on a total order of operations and prevents concurrent access from causing conflicts.
Many relational databases, such as PostgreSQL \cite{postgresql}, use this model.

However, strong consistency may be unwarranted or unneccessary depending on the application: it may impose an unacceptable performance degradation on the system, or it may simply be infeasible to implement, especially in large distributed systems.
Relying on a single leader or central server limits the use and deployment of these systems: the server may become a bottleneck that limits scalability, and it makes the system vulnerable to disruption by network outages, denial-of-service attacks, censorship, and server failures.
Clients must constantly communicate with the leader in order to perform operations; if a node cannot reach the leader due to a network fault, its execution is stalled.
This fact makes strong consistency unsuitable for mobile devices, such as laptops and smartphones, that have intermittent network connectivity and must work offline.
It also rules out approaches that bypass the central server by using a local network for replication.

By contrast, decentralised or peer-to-peer architectures with weaker consistency models are able to provide better performance, fault-tolerance, and scalability characteristics.
One widely-implemented model is \emph{eventual consistency}, which guarantees that if no new updates are made to the shared state, all nodes will eventually have the same data \cite{Bailis:2013jc,Burckhardt:2014hy,Terry:1994fp,Vogels:2009ca}.
Since this model allows conflicting updates to be made concurrently, it requires a mechanism for resolving such conflicts.
For example, version control systems such as Git or Mercurial require the user to resolve merge conflicts manually; and some ``NoSQL'' distributed database systems such as Cassandra adopt a \emph{last-writer-wins} policy, under which one update is chosen as the winner, and concurrent updates are discarded \cite{KingsburyCassandra}.
Eventual consistency offers weak guarantees: it does not constrain the system behaviour when updates never cease, or the values that read operations may return prior to convergence.

\emph{Strong eventual consistency} (SEC) is a model that strikes a compromise between strong and eventual consistency~\cite{Shapiro:2011un}.
Informally, it guarantees that whenever two nodes have received the same set of messages---possibly in a different order---their view of the shared state is identical, and any conflicting concurrent updates must be merged automatically.
Large-scale deployments of SEC algorithms include datacentre-based applications using the Riak distributed database \cite{Brown:2014hs}, and collaborative editing applications such as Google Docs \cite{DayRichter:2010tt}.

Unlike strong consistency models, it is possible to implement SEC in decentralised settings without any central server or leader, and it allows local execution at each node to proceed without waiting for communication with other nodes.
However, algorithms for achieving decentralised SEC are currently poorly understood: several such algorithms, published in peer-reviewed venues, were subsequently shown to violate their supposed guarantees \cite{Imine:2003ks,Imine:2006kn,Oster:2005vi}.
As we show in Section~\ref{sect.relatedwork}, informal reasoning has repeatedly produced plausible-looking but incorrect algorithms, and there have even been examples of mechanised formal proofs of SEC algorithm correctness later being shown to be flawed.
These mechanised proofs failed because, in formalising the algorithm, they made false assumptions about the execution environment.

In this work we use the Isabelle/HOL proof assistant~\cite{DBLP:conf/tphol/WenzelPN08} to create a framework for reliably reasoning about the correctness of a particular class of decentralised replication algorithms.
We do this by formalising not only the replication algorithms, but also the network in which they execute, allowing us to prove that the algorithm's assumptions hold in all possible network behaviours.
We model the network using the axioms of \emph{asynchronous unreliable causal broadcast}, a well-understood abstraction that is commonly implemented by network protocols, and which can run on almost any computer network, including large-scale networks that delay, reorder, or drop messages, and in which nodes may fail.

We then use this framework to produce machine-checked proofs of correctness for three Conflict-Free Replicated Data Types (CRDTs), a class of replication algorithms that ensure strong eventual consistency \cite{Shapiro:2011wy,Shapiro:2011un}.
These algorithms are suitable for use on mobile devices, which are not always connected to the Internet, but which may have a local connection (e.g. via Bluetooth) to other nodes carrying copies of the shared state.
We have used these algorithms to build a collaborative text editing application, and we plan to encapsulate them in a library that will allow developers to easily build applications that require data synchronisation, such as collaboratively editable spreadsheets, shared calendars, address books, and note-taking tools.

Our contributions in this paper are as follows:
\begin{itemize}
\item
We establish a framework for proving the strong eventual consistency (SEC) property of replication algorithms.
Our approach is ``foundational'' in the sense that we start with a general-purpose model of asynchronous unreliable causal broadcast networks---a communication abstraction that is compatible with virtually all network technologies today---and build up composable layers towards a full proof of correctness for a particular algorithm.
To our knowledge, this is the first machine-checked verification of SEC algorithms that explicitly models the network and reasons about all possible network behaviours.
The framework is modular and reusable, making it easy to formulate proofs for new algorithms.
\item
We provide the first mechanised proofs of the Replicated Growable Array, the operation-based Observed-Remove Set, and the operation-based counter CRDT.
These mechanised proofs demonstrate that our framework is highly reusable: we were able to quickly develop proofs of convergence for the set and counter CRDTs with little CRDT-specific code, by using a fixed proof pattern that applies to all of our CRDTs.
\item
As part of our proof framework, we identify an abstract convergence theorem, a property of order relations, from which we can deduce correctness theorems for concrete SEC algorithms.
Intuitively, this theorem can be viewed as the ``essence'' of why strong eventual consistency algorithms converge.
The convergence theorems for our three concrete CRDTs are obtained as direct corollaries of this theorem.
\end{itemize}

\input{proof-strategy}
\input{isabelle}
\input{convergence}
\input{network}
\input{rga}
\input{simple-crdts}
\input{relwork}
\section{Conclusion}
\label{sect.conclusion}

In this paper we introduced a new approach for proving the correctness of SEC algorithms: instead of defining axioms related to the basic properties of any individual algorithm, we instead build up from a formal, realistic model of a computer network which may delay, drop or reorder messages sent between computers.
We then complemented this model with an abstract specification of SEC.
We took this approach both to ease the burden of constructing proofs of new SEC algorithms, but also to address the difficulties with previous approaches where peer-reviewed proofs, including mechanised proofs, have later been shown to be incorrect.

We used our framework to show that three op-based CRDT algorithms do indeed provide the SEC property under all possible valid behaviours of our network model.
Our second and third proofs took only a few hours to write and under 100 lines of additional code, demonstrating that our framework is readily reusable.

All of our convergence proofs for our CRDT implementations follow a familiar recipe.
First local states, or a concrete implementation of the replicated type is defined along with a type of messages corresponding to the operations supported by the CRDT.
Together, these are coupled with an interpretation function which `decodes' messages and lifts them into a transformer of local states.
This transformer is used to define a CRDT-specific locale, a specialisation of the $\isa{network}{\isacharunderscore}\isa{with}{\isacharunderscore}(\isa{constrained}{\isacharunderscore})\isa{ops}$ locale, which may also provide restrictions on valid network behaviours in order to ensure convergence.
Next, all operations are shown to commute with themselves, and with each other, with preconditions on commutation being eventually dischargeable as a side-effect of the restrictions on network behaviours mentioned above.
Coupling our abstract convergence theorem with a CRDT-specific technical lemma showing that the concurrent delivered messages of a prefix of a local history of a node commute, it is easy to prove convergence.
In fact, the same single line of Isabelle proof is used to obtain the convergence theorem as a corollary, in all of our three concrete CRDT implementations.

As this recipe described above demonstrates, in this work we have not only isolated a series of reusable lemmas and models of networks, but also identified a fixed proof strategy that developers of new operation-based CRDTs can use to obtain a convergence theorem for their replicated type.
This idea is made precise with the $\isa{strong}{\isacharunderscore}\isa{eventual}{\isacharunderscore}\isa{consistency}$ local theory, which provides sufficient conditions in order to obtain a convergence theorem for a replicated type.

Our work was motivated by the desire to support a wider range of SEC algorithms in distributed systems.
OT algorithms which support the $\mathit{TP}_1$ alone are already widely used in systems such as Google Docs, however these require all clients communicate changes via a single central server.
Similarly, state-based CRDTs are used in systems such as Riak. Both of these approaches are limiting -- in the case of OT, the requirement for a central server prevents offline editing and direct collaboration via local WiFi; and in the case of state-based CRDTs, algorithms to support complex data structures such as ordered lists are yet to be found and in any case are likely impractical if small changes are regularly made to lists containing many items.
Consequently, our approach provides the groundwork for a new generation of applications which use truly distributed SEC alogrithms, including robust, collaborative applications working on complex data structures in a peer-to-peer setting, something which is likely to become increasingly important in a world where mobile devices are becoming ever more prevalent.

We speculate that our framework is also amenable to the inclusion of SEC algorithms based on OT as well as state-based CRDTs, and not limited to operation-based algorithms, which we have focussed on in this work.
Further, we also speculate that our framework could be used to demonstrate the equivalence of classes of SEC algorithms.
We leave this to future work.

%% contents of acknowledgement section are automatically suppressed when the 'anonymous' documentclass option is set
\begin{acks}
    %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and \grantnum[<url>]{<sponsorID>}{<number>}
    %% should be used to acknowledge financial support and will be used by metadata extraction tools.
    The authors wish to acknowledge the support of
    \grantsponsor{GS100000003}{The Boeing Company}{http://dx.doi.org/10.13039/100000003} and
    \grantsponsor{GS501100000266}{EPSRC}{http://dx.doi.org/10.13039/501100000266} and
    \grantsponsor{EP/K008528}{``REMS: Rigorous Engineering for Mainstream Systems'' EPSRC Programme Grant (EP/K008528)}{http://dx.doi.org/10.13039/501100000266}.
We thank Peter Sewell and Alan Mycroft for constructive comments on an earlier version of this paper.

    %TODO: REMS, Colleagues who have provided feedback.
\end{acks}

\bibliography{references}{}
\end{document}
