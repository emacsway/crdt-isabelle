\documentclass[acmlarge,review,anonymous]{acmart}\settopmatter{printfolios=true}
%\documentclass[acmlarge]{acmart}\settopmatter{}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}
\usepackage[english]{babel}
\usepackage{setspace}

\usepackage{paralist} % For inline enumeration
\usepackage{tikz} % For diagrams
\usetikzlibrary{arrows}

\usepackage{isabelle,isabellesym}
\isabellestyle{it}

\newif\ifextended
\extendedfalse

\hyphenation{App-Jet}

%% PACMPL journal information will be supplied by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{--}
\acmNumber{--}
\acmArticle{0}
\acmYear{2017}
\acmMonth{4}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\setcopyright{none}

\begin{document}
\title{Verifying Strong Eventual Consistency in Distributed Systems}

%% Each author should be introduced by \author, followed by \authornote (optional),
%% \orcid (optional), \affiliation, and \email.
%% An author may have multiple affiliations and/or emails; repeat the appropriate command.
%% Many elements are not rendered, but should be provided for metadata extraction tools.
\author{Victor B.\ F.\ Gomes}
%\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{vb358@cam.ac.uk}

\author{Martin Kleppmann}
\orcid{0000-0001-7252-6958}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{mk428@cam.ac.uk}

\author{Dominic P.\ Mulligan}
%\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Research Associate}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{dpm36@cam.ac.uk}

\author{Alastair R.\ Beresford}
\orcid{0000-0003-0818-6535}
\affiliation{
  \position{Senior Lecturer}
  \department{Computer Laboratory}
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}
\email{arb33@cam.ac.uk}

\begin{abstract}
Data replication is used in distributed systems to maintain an up-to-date copy of a data structure across multiple computers in a network, with a variety of algorithms in existence which explore the trade-off between varying levels of data consistency versus operational constraints and system performance.
However, despite decades of research, algorithms for achieving replication in distributed systems are still poorly understood.
Indeed, many published algorithms have later been shown to be incorrect, even those accompanied by supposed mechanised proofs of correctness.

In this work, we focus on the correctness of Conflict-free Replicated Data Types (CRDTs), a class of algorithm that provides Strong Eventual Consistency guarantees for replicated data.
We provide a modular and reusable framework in the Isabelle/HOL interactive theorem prover for verifying the correctness of CRDT implementations.
We sidestep correctness issues that have dogged previous mechanised proofs in this area by axiomatically modelling a large class of networks---the asynchronous causal networks---using five uncontroversial axioms and incrementally building up in composable layers, obtaining a machine-checked assurance that our theorems hold in all possible network behaviours.
We identify an abstract convergence theorem, a property of order relations, with which we obtain correctness theorems for three concrete implementations---the Replicated Growable Array, the Observed-Removed Set, and a increment-decrement counter---as corollaries with a thin-layer of CRDT-specific code, validating our claim to have created a framework for CRDT verification.
\end{abstract}

%\begin{abstract}
%Data replication is used in distributed systems to maintain an up-to-date copy of a data structure across multiple computers, with a variety of data replication algorithms in existence which explore the trade-off between varying levels of data consistency across computers versus operational constraints and system performance.
%In this paper we focus on Conflict-free Replicated Data Types (CRDTs), a class of algorithm which provide Strong Eventual Consistency (SEC).
%These algorithms are worthy of study not only because they are widely used today, but also because previous peer-reviewed SEC algorithms have later turned out to be incorrect, including those which claimed to possess a mechanised proof of correctness.
%Such past failures occurred because the axioms used in proofs were subsequently shown to contain subtle errors.
%The core difficulty in designing SEC algorithms arises from the fact that computer networks may delay, drop and reorder messages sent between computers.
%We therefore construct a realistic, formal model of how a computer network enables communication between machines in a distributed system using just five uncontroversial axioms.
%We implement our framework using the interactive theorem prover Isabelle/HOL, and use it to prove the correctness of SEC algorithms across all possible combinations of message delays, re-orderings and deletions.
%In particular, we provide the first mechanised proofs of convergence for counter, set and ordered-list CRDTs.
%\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003033.10003039.10003041.10003042</concept_id>
<concept_desc>Networks~Protocol testing and verification</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010537.10010540</concept_id>
<concept_desc>Computer systems organization~Peer-to-peer architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003033.10003039.10003041.10003043</concept_id>
<concept_desc>Networks~Formal specifications</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010172</concept_id>
<concept_desc>Theory of computation~Distributed algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10010124.10010138.10010142</concept_id>
<concept_desc>Theory of computation~Program verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011099.10011692</concept_id>
<concept_desc>Software and its engineering~Formal software verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Networks~Protocol testing and verification}
\ccsdesc[500]{Computer systems organization~Peer-to-peer architectures}
\ccsdesc[300]{Networks~Formal specifications}
\ccsdesc[300]{Theory of computation~Distributed algorithms}
\ccsdesc[300]{Theory of computation~Program verification}
\ccsdesc[300]{Software and its engineering~Formal software verification}

\keywords{strong eventual consistency, verification, distributed systems, replication, convergence, CRDTs, automated theorem proving}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sect.introduction}

Data \emph{replication} is an important and challenging task in a distributed system: given some data or shared state, a replication algorithm executed by a set of computers or \emph{nodes} ensures all such nodes end up with an identical copy of the data.
Such algorithms achieve this despite computer networks which delay, drop and reorder messages; networks which may be temporarily partitioned into two or more groups such that nodes in one group cannot communicate with nodes in another; and a subset of nodes executing the algorithm that may permanently fail.

A variety of replication algorithms exist which explore the trade-off between varying levels of data consistency across nodes versus operational constraints and system performance.
Such algorithms can be divided into three classes: strong consistency, eventual consistency and strong eventual consistency.
Strong consistency can be understood as linearizability: the system behaves as if there were only a single copy of the data, even when it is replicated; examples include traditional relational databases such as PostgreSQL.
Eventual consistency guarantees that, when no new updates are made to the shared state, all nodes eventually agree on the contents of the shared state; examples include distributed version control systems such as Git.
Strong eventual consistency provides better guarantees than eventual consistency as follows: whenever two nodes have received the same set of messages (possibly in a different order), then the shared state on the two nodes will be identical; examples include collaborative editing systems such as Google Docs, or data centre applications built with Riak.

In this paper we focus on replication algorithms which support decentralised operation and allow modifications to shared state in the presence of arbitrary network partitions.
Such algorithms support apps running on mobile devices such as laptops and smartphones which are not always connected to the Internet, but are instead completely disconnected or only have a local connection (e.g. via Bluetooth) to a subset of the nodes containing the shared state.
Example apps built using such algorithms could include collaborative editing systems such as text documents or spreadsheets where several users need to work on the same document, or data synchronisation tasks such as shared calendars, address books or note-taking tools.

We argue that such systems are best designed with replication algorithms which provide strong eventual consistency. Strong consistency algorithms are no good because they cannot support local updates on nodes in the presence of arbitrary network partitions.
In other words, editing of a spreadsheet by a node would not be allowed without network connectivity to a majority of nodes.
Eventual consistency is not ideal because it requires the user to manually resolve merge conflicts.
In contrast replication algorithms which support strong eventual consistency work in a distributed setting and are, by definition, conflict free.

Nevertheless, eventual consistency is how many popular collaborative and data synchronisation apps work today: allow
conflicts to arise and provide the user with mechanisms for resolving them.
The other common solution is to use a strong eventual consistency algorithm with a central server, for example, as popularised by Google Docs.
This approach does not support offline editing: it requires the user's computer to be connected to the server in order to ensure updates are handled in a conflict-free manner.
Furthermore, it requires users trust that the central server is not compromised by attackers or subverted by a malicious insider who could tamper with the data or grant access to unauthorised parties.
Finally, a central server is a single point of failure that is susceptible to denial-of-service attacks, blocking and censorship.

Decentralised, or peer-to-peer systems are attractive in scenarios where reliance on a central server is impractical or undesirable.
This includes both the mobile app scenarios described above, as well as data centre settings in which a central server is a bottleneck, limiting scalability.
Unfortunately, we currently have a poor understanding of algorithms that enable strong eventual consistency in this setting.
In Section~\ref{sect.relatedwork} we highlight several decentralised strong eventual consistency algorithms, published in peer-reviewed venues, that claimed to work correctly but were subsequently shown to violate their supposed guarantees. 
Informal reasoning has resulted in the development of algorithms later shown incorrect; even formal, mechanised proofs of algorithms have later been shown incorrect.
These proofs failed because the proofs were shallow: axioms which were thought to be correct were later found to be wrong.

Given the failure of previous mechanised proofs we take a different approach: rather than writing axioms which talk about any given strong eventual consistency algorithm we write down the axioms of a realistic class of computer network, the \emph{asynchronous causal network}.
This network model describes the behviour of large-scale computer networks today, which may reorder and delay message delivery, simulating temporary network partitions, as well as drop messages entirely, simulating permanent network and computer failure.
We use Isabelle/HOL, a generic proof assistant tool~\cite{DBLP:conf/tphol/WenzelPN08}, to create formal specifications of the network and distributed algorithms executing in such a system. 
We then use this framework to produce machine-checked proofs of correctness of a class of strong eventual consistency algorithms called Conflict-Free Replicated Data Types (CRDT) as introduced by~\citet{Shapiro:2011wy,Shapiro:2011un}. Our ultimate goal is to formally verify the correctness of a suite of distributed data types which can be composed and encapsulated in a library for use by application developers, thus providing an easy way to produce apps which work in a distributed setting and providing end users with a new generation of conflict-free collaborative and data synchronising apps.

Our contributions in this paper are as follows:
\begin{itemize}
\item We outline a novel approach for proving the correctness of strong eventual consistency algorithms, namely by building a model of asynchronous causal networks, a communication paradigm supported by virtually all computer network technologies today.
%
\item Develop the first, modular and reusable framework for verifying the correctness of strong eventual consistency algorithms, obtaining a machine-checked assurance that our theorems hold in all possible network behaviours.
%
\item Identify an abstract convergence theorem, a property of order relations, with which we can obtain correctness theorems for concrete strong eventual consistency algorithms.
%
\item We provide the first mechanised proofs of the Replicated Growable Array, the Observed-Removed Set and increment-decrement counter CRDTs. 
%
\item We demonstrate our framework is highly reusable: we developed proofs of the Observed-Removed Set and a increment-decrement counter CRDTs in a few hours and under 100 lines of additional code each.
\end{itemize}

\input{background}
\input{relwork}
\input{isabelle}
\input{convergence}
\input{network}
\input{rga}
\input{simple-crdts}

\section{Conclusion}
\label{sect.conclusion}

In this paper we introduced a new approach for proving the correctness of SEC algorithms: instead of defining axioms related to the basic properties of any individual algorithm, we instead build up from a formal, realistic model of a computer network which may delay, drop or reorder messages sent between computers.
We then complemented this model with an abstract specification of SEC.
We took this approach both to ease the burden of constructing proofs of new SEC algorithms, but also to address the difficulties with previous approaches where peer-reviewed proofs, including mechanised proofs, have later been shown to be incorrect.

We used our framework to show that three op-based CRDT algorithms do indeed provide the SEC property under all possible valid executions of our network model.
Our second and third proofs took only a few hours to write and under 100 lines of additional code, demonstrating that our framework is readily reusable.

All of our convergence proofs for our CRDT implementations follow a familiar recipe.
First local states, or a concrete implementation of the replicated type is defined along with a type of messages corresponding to the operations supported by the CRDT.
Together, these are coupled with an interpretation function which `decodes' messages and lifts them into a transformer of local states.
This transformer is used to define a CRDT-specific locale, a specialisation of the $\isa{network}{\isacharunderscore}\isa{with}{\isacharunderscore}(\isa{constrained}{\isacharunderscore})\isa{ops}$ locale, which may also provide restrictions on valid network behaviours in order to ensure convergence.
Next, all operations are shown to commute with themselves, and with each other, with preconditions on commutation being eventually dischargeable as a side-effect of the restrictions on network behaviours mentioned above.
Coupling our abstract convergence theorem with a CRDT-specific technical lemma showing that the concurrent delivered messages of a prefix of a local history of a node commute, it is easy to prove convergence.
In fact, the same single line of Isabelle proof is used to obtain the convergence theorem as a corollary, in all of our three concrete CRDT implementations.

As this recipe described above demonstrates, in this work we have not only isolated a series of reusable lemmas and models of networks, but also identified a fixed proof strategy that developers of new operation-based CRDTs can use to obtain a convergence theorem for their replicated type.
This idea is made precise with the $\isa{strong}{\isacharunderscore}\isa{eventual}{\isacharunderscore}\isa{consistency}$ local theory, which provides sufficient conditions in order to obtain a convergence theorem for a replicated type.

Our work was motivated by the desire to support a wider range of SEC algorithms in distributed systems.
OT algorithms which support the $\mathit{TP}_1$ alone are already widely used in systems such as Google Docs, however these require all clients communicate changes via a single central server.
Similarly, state-based CRDTs are used in systems such as Riak. Both of these approaches are limiting -- in the case of OT, the requirement for a central server prevents offline editing and direct collaboration via local WiFi; and in the case of state-based CRDTs, algorithms to support complex data structures such as ordered lists are yet to be found and in any case are likely impractical if small changes are regularly made to lists containing many items.
Consequently, our approach provides the groundwork for a new generation of applications which use truly distributed SEC alogrithms, including robust, collaborative applications working on complex data structures in a peer-to-peer setting, something which is likely to become increasingly important in a world where mobile devices are becoming ever more prevalent.

We speculate that our framework is also amenable to the inclusion of SEC algorithms based on OT as well as state-based CRDTs, and not limited to operation-based algorithms, which we have focussed on in this work.
Further, we also speculate that our framework could be used to demonstrate the equivalence of classes of SEC algorithms.
We leave this to future work.

%% contents of acknowledgement section are automatically suppressed when the 'anonymous' documentclass option is set
\begin{acks}
    %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and \grantnum[<url>]{<sponsorID>}{<number>}
    %% should be used to acknowledge financial support and will be used by metadata extraction tools.
    This research was supported by grants from
    \grantsponsor{GS100000003}{The Boeing Company}{http://dx.doi.org/10.13039/100000003} and
    \grantsponsor{GS501100000266}{EPSRC}{http://dx.doi.org/10.13039/501100000266} and
    \grantsponsor{EP/K008528}{EPSRC}{http://dx.doi.org/10.13039/501100000266}

    %TODO: REMS, Colleagues who have provided feedback.
\end{acks}

\bibliography{references}{}
\end{document}
